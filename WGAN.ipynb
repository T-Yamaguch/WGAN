{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/WGAN/blob/master/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "outputId": "f2021a80-75e6-4143-a869-ea742fd8f9a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCaY7uIeBtcK",
        "outputId": "cd6b13d2-1c83-432b-8eb0-66e055f144dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct  1 11:39:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class up_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(up_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = LeakyReLU()\n",
        "    self.act2 = LeakyReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class down_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(down_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.drop = Dropout(0.3)\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "outputId": "d802f82e-3158-499e-c2f5-ccae2ee04619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 256\n",
        "    self.layer_num = 5\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    # while data_size*64 < final_size:\n",
        "    #   data_size *= 64\n",
        "    #   x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num /= 2\n",
        "      x = up_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 4096)              53248     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_block (up_block)          (None, 8, 8, 128)         819840    \n",
            "_________________________________________________________________\n",
            "up_block_1 (up_block)        (None, 16, 16, 64)        205120    \n",
            "_________________________________________________________________\n",
            "up_block_2 (up_block)        (None, 32, 32, 32)        51360     \n",
            "_________________________________________________________________\n",
            "up_block_3 (up_block)        (None, 64, 64, 16)        12880     \n",
            "_________________________________________________________________\n",
            "up_block_4 (up_block)        (None, 128, 128, 8)       3240      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 128, 128, 3)       603       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 1,146,291\n",
            "Trainable params: 1,137,603\n",
            "Non-trainable params: 8,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "outputId": "aee1c67c-fa29-4db6-b665-86b57d9eb5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 8\n",
        "    self.layer_num = 5\n",
        "    self.input_shape = (128, 128, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer, batch_norm = False)(x)\n",
        "      x = down_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "    x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Flatten()(x)\n",
        "    x =  Dense(8, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    outputs =  Dense(1, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "d.model().summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "down_block (down_block)      (None, 64, 64, 8)         640       \n",
            "_________________________________________________________________\n",
            "down_block_1 (down_block)    (None, 32, 32, 16)        3280      \n",
            "_________________________________________________________________\n",
            "down_block_2 (down_block)    (None, 16, 16, 32)        12960     \n",
            "_________________________________________________________________\n",
            "down_block_3 (down_block)    (None, 8, 8, 64)          51520     \n",
            "_________________________________________________________________\n",
            "down_block_4 (down_block)    (None, 4, 4, 128)         205440    \n",
            "_________________________________________________________________\n",
            "conv_block (conv_block)      (None, 4, 4, 256)         820480    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 32776     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,127,105\n",
            "Trainable params: 1,126,097\n",
            "Non-trainable params: 1,008\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj",
        "outputId": "46495e43-5db7-44ae-c277-e648c3c8e39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "class WGAN():\n",
        "  def __init__(self, \n",
        "               img_size= 128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/WGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'drive/My Drive/samples/image'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image/255  # normalize to [0,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def discriminator_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    loss_d = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    return loss_d\n",
        "\n",
        "  def generator_loss(self, generated_outputs):\n",
        "    loss_g = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return loss_g\n",
        "\n",
        "  def mse_loss(self, true, pred):\n",
        "    loss =  tf.math.reduce_mean(MSE(true, pred))\n",
        "    return loss\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array(gen_img*255, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/WGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=200)\n",
        "    # plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/WGAN/loss/gan_loss.png')\n",
        "    plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    d_batch_loss = sum(self.d_temp)/len(self.d_temp)\n",
        "    g_batch_loss = sum(self.g_temp)/len(self.g_temp)\n",
        "    print ('\\nd loss: {}, g loss: {}'.format(d_batch_loss, g_batch_loss))\n",
        "    self.d_history.append(d_batch_loss)\n",
        "    self.g_history.append(g_batch_loss)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "    [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.d_train(imgs)\n",
        "        [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "        if steps % self.n_critics == 0:\n",
        "          self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = WGAN(img_size = 128,\n",
        "           code_num = 8,\n",
        "           batch_size = 256,\n",
        "           train_epochs = 10000, \n",
        "           train_steps = 8, \n",
        "           checkpoint_epochs = 100, \n",
        "           image_epochs = 10, \n",
        "           start_epoch = 1,\n",
        "           optimizer = RMSprop(lr=5E-5),\n",
        "           n_critics = 1\n",
        "           )\n",
        "  a()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epochs 1\n",
            "steps8\n",
            "d loss: -1.7901292892474885e-08, g loss: 8.844667718221899e-07\n",
            "\n",
            "epochs 2\n",
            "steps8\n",
            "d loss: 1.4953982940824062e-08, g loss: 5.722513378714211e-07\n",
            "\n",
            "epochs 3\n",
            "steps8\n",
            "d loss: -5.098034705497412e-08, g loss: 1.2246338201293838e-06\n",
            "\n",
            "epochs 4\n",
            "steps8\n",
            "d loss: 1.48991929904696e-10, g loss: 1.0278521358486614e-06\n",
            "\n",
            "epochs 5\n",
            "steps8\n",
            "d loss: -1.5505125361414684e-07, g loss: 3.4530908123997506e-06\n",
            "\n",
            "epochs 6\n",
            "steps8\n",
            "d loss: 5.26284793522791e-10, g loss: 4.017012543044984e-06\n",
            "\n",
            "epochs 7\n",
            "steps8\n",
            "d loss: 4.1078010326600634e-08, g loss: 3.858546733681578e-06\n",
            "\n",
            "epochs 8\n",
            "steps8\n",
            "d loss: -1.3279543509270297e-07, g loss: 3.324643330415711e-06\n",
            "\n",
            "epochs 9\n",
            "steps8\n",
            "d loss: -2.2982735004006827e-07, g loss: 3.4035872431559255e-06\n",
            "\n",
            "epochs 10\n",
            "steps8\n",
            "d loss: -2.9012210234213853e-07, g loss: 2.0840725483139977e-06\n",
            "\n",
            "epochs 11\n",
            "steps8\n",
            "d loss: 8.085706326710351e-09, g loss: -6.38238475403341e-07\n",
            "\n",
            "epochs 12\n",
            "steps8\n",
            "d loss: 1.3452197578089908e-08, g loss: 4.6971223355285474e-07\n",
            "\n",
            "epochs 13\n",
            "steps8\n",
            "d loss: -5.3551872269963496e-08, g loss: -2.2378773678610742e-07\n",
            "\n",
            "epochs 14\n",
            "steps8\n",
            "d loss: 2.142671462479484e-07, g loss: -1.8722121239989065e-06\n",
            "\n",
            "epochs 15\n",
            "steps8\n",
            "d loss: 4.407829123920237e-08, g loss: -3.4205668271169998e-06\n",
            "\n",
            "epochs 16\n",
            "steps8\n",
            "d loss: 1.52191091729037e-07, g loss: -3.990341610915493e-06\n",
            "\n",
            "epochs 17\n",
            "steps8\n",
            "d loss: -2.2134884147817502e-08, g loss: -4.2483188735786825e-06\n",
            "\n",
            "epochs 18\n",
            "steps8\n",
            "d loss: -1.3397121279012936e-07, g loss: -5.402422630140791e-06\n",
            "\n",
            "epochs 19\n",
            "steps8\n",
            "d loss: 1.0273151929141022e-07, g loss: -5.983432856737636e-06\n",
            "\n",
            "epochs 20\n",
            "steps8\n",
            "d loss: 3.1344859507953515e-07, g loss: -4.376840934128268e-06\n",
            "\n",
            "epochs 21\n",
            "steps8\n",
            "d loss: 1.1985611081399838e-07, g loss: -3.665349368020543e-06\n",
            "\n",
            "epochs 22\n",
            "steps8\n",
            "d loss: -1.8254996803079848e-07, g loss: -4.685611202148721e-06\n",
            "\n",
            "epochs 23\n",
            "steps8\n",
            "d loss: 9.989395266529755e-08, g loss: -4.204399829177419e-06\n",
            "\n",
            "epochs 24\n",
            "steps8\n",
            "d loss: -1.0629207736201352e-07, g loss: -6.467129878728883e-06\n",
            "\n",
            "epochs 25\n",
            "steps8\n",
            "d loss: 3.091562348345178e-07, g loss: -7.764464498905e-06\n",
            "\n",
            "epochs 26\n",
            "steps8\n",
            "d loss: -4.462282277017948e-08, g loss: -4.564459231914952e-06\n",
            "\n",
            "epochs 27\n",
            "steps8\n",
            "d loss: -1.221849856847257e-07, g loss: -5.359717761166394e-06\n",
            "\n",
            "epochs 28\n",
            "steps8\n",
            "d loss: 4.6742684389755595e-08, g loss: -5.055029305367498e-06\n",
            "\n",
            "epochs 29\n",
            "steps8\n",
            "d loss: 2.8060716772415617e-07, g loss: -3.676301730592968e-06\n",
            "\n",
            "epochs 30\n",
            "steps8\n",
            "d loss: 8.316091282267735e-08, g loss: -2.788456868074718e-06\n",
            "\n",
            "epochs 31\n",
            "steps8\n",
            "d loss: 2.2766937490814598e-07, g loss: -8.016905326257984e-07\n",
            "\n",
            "epochs 32\n",
            "steps8\n",
            "d loss: 4.713946566425875e-09, g loss: 8.504021025146358e-07\n",
            "\n",
            "epochs 33\n",
            "steps8\n",
            "d loss: -1.594969347706865e-07, g loss: -3.691296228680585e-07\n",
            "\n",
            "epochs 34\n",
            "steps8\n",
            "d loss: 9.000016376603526e-08, g loss: 3.678779023630341e-07\n",
            "\n",
            "epochs 35\n",
            "steps8\n",
            "d loss: 1.4982710183630843e-07, g loss: -7.839422551114694e-07\n",
            "\n",
            "epochs 36\n",
            "steps8\n",
            "d loss: -1.0129546978987491e-07, g loss: -3.1300839964387706e-06\n",
            "\n",
            "epochs 37\n",
            "steps8\n",
            "d loss: 8.382454552702256e-08, g loss: -3.8792086343164556e-06\n",
            "\n",
            "epochs 38\n",
            "steps8\n",
            "d loss: -2.495766580068448e-07, g loss: -4.348533821030287e-06\n",
            "\n",
            "epochs 39\n",
            "steps8\n",
            "d loss: 1.0806161299115047e-07, g loss: -4.081912720721448e-06\n",
            "\n",
            "epochs 40\n",
            "steps8\n",
            "d loss: 3.2575542263657553e-07, g loss: -3.0425869681494078e-06\n",
            "\n",
            "epochs 41\n",
            "steps8\n",
            "d loss: -1.7879635549888917e-07, g loss: -1.7675612298262422e-06\n",
            "\n",
            "epochs 42\n",
            "steps8\n",
            "d loss: 6.76453666415e-08, g loss: -4.446596904017497e-07\n",
            "\n",
            "epochs 43\n",
            "steps8\n",
            "d loss: 3.0821192353869264e-07, g loss: -1.2685158026215504e-06\n",
            "\n",
            "epochs 44\n",
            "steps8\n",
            "d loss: -2.7643675792887734e-08, g loss: -1.32077002490405e-06\n",
            "\n",
            "epochs 45\n",
            "steps8\n",
            "d loss: 7.561228443364598e-08, g loss: -8.455720603706141e-07\n",
            "\n",
            "epochs 46\n",
            "steps8\n",
            "d loss: 1.0767102764930314e-08, g loss: 4.134339519623609e-07\n",
            "\n",
            "epochs 47\n",
            "steps8\n",
            "d loss: 1.490428616079953e-07, g loss: 2.2425629140343517e-06\n",
            "\n",
            "epochs 48\n",
            "steps8\n",
            "d loss: 5.045140483161958e-07, g loss: 2.481792989783571e-06\n",
            "\n",
            "epochs 49\n",
            "steps8\n",
            "d loss: -3.0643934678664664e-07, g loss: 2.9562311283370946e-06\n",
            "\n",
            "epochs 50\n",
            "steps8\n",
            "d loss: 1.92607984672577e-07, g loss: 3.6718729461426847e-06\n",
            "\n",
            "epochs 51\n",
            "steps8\n",
            "d loss: -7.660764822503552e-08, g loss: 4.229152636980871e-06\n",
            "\n",
            "epochs 52\n",
            "steps8\n",
            "d loss: -1.2342079003246909e-07, g loss: 3.10603218167671e-06\n",
            "\n",
            "epochs 53\n",
            "steps8\n",
            "d loss: -1.9034018805541564e-07, g loss: 3.127453510387568e-06\n",
            "\n",
            "epochs 54\n",
            "steps8\n",
            "d loss: -1.3084175520816643e-07, g loss: 2.9628311040141853e-06\n",
            "\n",
            "epochs 55\n",
            "steps8\n",
            "d loss: -5.958301585451409e-08, g loss: 4.109560904907994e-06\n",
            "\n",
            "epochs 56\n",
            "steps8\n",
            "d loss: -1.55736046281163e-07, g loss: 4.812957740796264e-06\n",
            "\n",
            "epochs 57\n",
            "steps8\n",
            "d loss: -4.4249532038520556e-08, g loss: 5.346806119632674e-06\n",
            "\n",
            "epochs 58\n",
            "steps8\n",
            "d loss: -2.1138157535460778e-07, g loss: 5.895959475310519e-06\n",
            "\n",
            "epochs 59\n",
            "steps8\n",
            "d loss: 1.9500697590046912e-07, g loss: 3.865922735712957e-06\n",
            "\n",
            "epochs 60\n",
            "steps8\n",
            "d loss: -7.319613359868526e-08, g loss: 3.7301003885659156e-06\n",
            "\n",
            "epochs 61\n",
            "steps8\n",
            "d loss: 2.8728695156132744e-07, g loss: 2.983845661219675e-06\n",
            "\n",
            "epochs 62\n",
            "steps8\n",
            "d loss: -4.856924533669371e-07, g loss: 2.974927383547765e-06\n",
            "\n",
            "epochs 63\n",
            "steps8\n",
            "d loss: -2.748511747086013e-07, g loss: 3.305187192381709e-06\n",
            "\n",
            "epochs 64\n",
            "steps8\n",
            "d loss: 1.5005980458226986e-08, g loss: 2.301707809237996e-06\n",
            "\n",
            "epochs 65\n",
            "steps8\n",
            "d loss: -4.653287533074035e-08, g loss: 2.6012030502897687e-06\n",
            "\n",
            "epochs 66\n",
            "steps8\n",
            "d loss: -1.8625541997607797e-08, g loss: 2.709486125240801e-06\n",
            "\n",
            "epochs 67\n",
            "steps8\n",
            "d loss: 3.411261673136323e-07, g loss: 3.719829919646145e-06\n",
            "\n",
            "epochs 68\n",
            "steps8\n",
            "d loss: 8.805309903436864e-08, g loss: 3.986564934166381e-06\n",
            "\n",
            "epochs 69\n",
            "steps8\n",
            "d loss: 5.016172508476302e-07, g loss: 3.748864173758193e-06\n",
            "\n",
            "epochs 70\n",
            "steps8\n",
            "d loss: 2.818836719598039e-08, g loss: 3.121567488051369e-06\n",
            "\n",
            "epochs 71\n",
            "steps8\n",
            "d loss: -1.3115193553403515e-07, g loss: 2.1901623767917044e-06\n",
            "\n",
            "epochs 72\n",
            "steps8\n",
            "d loss: 1.6525493862218354e-08, g loss: 4.107158702026936e-07\n",
            "\n",
            "epochs 73\n",
            "steps8\n",
            "d loss: 4.6416869281529216e-07, g loss: -1.3704334378417116e-06\n",
            "\n",
            "epochs 74\n",
            "steps8\n",
            "d loss: 1.1248644682382292e-08, g loss: -1.2445520951587241e-06\n",
            "\n",
            "epochs 75\n",
            "steps8\n",
            "d loss: 3.5948175991507014e-07, g loss: -8.383522640542651e-07\n",
            "\n",
            "epochs 76\n",
            "steps8\n",
            "d loss: 1.9135438833473017e-07, g loss: 1.8262657874856814e-07\n",
            "\n",
            "epochs 77\n",
            "steps8\n",
            "d loss: 1.1401901645058388e-07, g loss: 1.6785008938313695e-07\n",
            "\n",
            "epochs 78\n",
            "steps8\n",
            "d loss: 2.9387439326455933e-07, g loss: -3.5937259212914796e-07\n",
            "\n",
            "epochs 79\n",
            "steps8\n",
            "d loss: -1.1736635485704028e-07, g loss: 1.1937420367758023e-06\n",
            "\n",
            "epochs 80\n",
            "steps8\n",
            "d loss: 2.7242379019298824e-07, g loss: 1.6239888509517186e-06\n",
            "\n",
            "epochs 81\n",
            "steps8\n",
            "d loss: 2.4437562728962803e-07, g loss: 1.2358298135950463e-06\n",
            "\n",
            "epochs 82\n",
            "steps8\n",
            "d loss: 3.1407427059093607e-07, g loss: 5.101560418552253e-07\n",
            "\n",
            "epochs 83\n",
            "steps8\n",
            "d loss: 2.3390869330341957e-07, g loss: 3.124618501715304e-07\n",
            "\n",
            "epochs 84\n",
            "steps8\n",
            "d loss: -7.483779995709483e-08, g loss: -4.967600943928119e-07\n",
            "\n",
            "epochs 85\n",
            "steps8\n",
            "d loss: -2.6400243768875953e-08, g loss: 1.1232532415306196e-06\n",
            "\n",
            "epochs 86\n",
            "steps8\n",
            "d loss: 2.439351192151662e-07, g loss: 1.7623694930080092e-06\n",
            "\n",
            "epochs 87\n",
            "steps8\n",
            "d loss: -2.553527167492575e-07, g loss: 1.9462572709016968e-06\n",
            "\n",
            "epochs 88\n",
            "steps8\n",
            "d loss: 9.599281725058972e-08, g loss: 2.596446847746847e-06\n",
            "\n",
            "epochs 89\n",
            "steps8\n",
            "d loss: -3.2100177804750274e-07, g loss: 4.29572310167714e-06\n",
            "\n",
            "epochs 90\n",
            "steps8\n",
            "d loss: -3.414202183193993e-07, g loss: 4.495496341405669e-06\n",
            "\n",
            "epochs 91\n",
            "steps8\n",
            "d loss: 2.35681199001192e-07, g loss: 4.273675585864112e-06\n",
            "\n",
            "epochs 92\n",
            "steps8\n",
            "d loss: -5.326734253685572e-08, g loss: 6.155836217658361e-06\n",
            "\n",
            "epochs 93\n",
            "steps8\n",
            "d loss: 1.9017704744328512e-08, g loss: 6.6191869336762466e-06\n",
            "\n",
            "epochs 94\n",
            "steps8\n",
            "d loss: -1.0636313163558953e-07, g loss: 6.811892944824649e-06\n",
            "\n",
            "epochs 95\n",
            "steps8\n",
            "d loss: 4.930026307192747e-07, g loss: 6.161417331895791e-06\n",
            "\n",
            "epochs 96\n",
            "steps8\n",
            "d loss: 5.0956543873326154e-08, g loss: 4.962226284987992e-06\n",
            "\n",
            "epochs 97\n",
            "steps8\n",
            "d loss: 1.2793435644198325e-08, g loss: 4.4987955334363505e-06\n",
            "\n",
            "epochs 98\n",
            "steps8\n",
            "d loss: -1.1110466857644496e-07, g loss: 5.2442974265431985e-06\n",
            "\n",
            "epochs 99\n",
            "steps8\n",
            "d loss: -3.133748691652727e-07, g loss: 7.373939752142178e-06\n",
            "\n",
            "epochs 100\n",
            "steps8\n",
            "d loss: 4.3538136651477544e-08, g loss: 6.435439900087658e-06\n",
            "\n",
            "Saving checkpoint at epoch100\n",
            "\n",
            "\n",
            "\n",
            "epochs 101\n",
            "steps8\n",
            "d loss: -1.9319742250445415e-08, g loss: 4.490090304898331e-06\n",
            "\n",
            "epochs 102\n",
            "steps8\n",
            "d loss: -8.326412626047386e-08, g loss: 4.981214715371607e-06\n",
            "\n",
            "epochs 103\n",
            "steps8\n",
            "d loss: 3.1037507142173126e-07, g loss: 3.0478224744001636e-06\n",
            "\n",
            "epochs 104\n",
            "steps8\n",
            "d loss: 1.1550358181011688e-07, g loss: 3.556185220077168e-06\n",
            "\n",
            "epochs 105\n",
            "steps8\n",
            "d loss: 7.038891567390237e-07, g loss: 5.267915639706189e-06\n",
            "\n",
            "epochs 106\n",
            "steps8\n",
            "d loss: -1.267773086510715e-07, g loss: 4.26873975811759e-06\n",
            "\n",
            "epochs 107\n",
            "steps8\n",
            "d loss: -5.0347352953394875e-09, g loss: 5.825308107887395e-06\n",
            "\n",
            "epochs 108\n",
            "steps8\n",
            "d loss: -1.794836634871899e-08, g loss: 8.159280696418136e-06\n",
            "\n",
            "epochs 109\n",
            "steps8\n",
            "d loss: -4.6533727982023265e-08, g loss: 1.0091387594002299e-05\n",
            "\n",
            "epochs 110\n",
            "steps8\n",
            "d loss: 3.171315938743646e-07, g loss: 1.1260529390710872e-05\n",
            "\n",
            "epochs 111\n",
            "steps8\n",
            "d loss: 1.8096898202202283e-08, g loss: 1.1148128578497563e-05\n",
            "\n",
            "epochs 112\n",
            "steps8\n",
            "d loss: -1.867857690740493e-07, g loss: 1.1925110811716877e-05\n",
            "\n",
            "epochs 113\n",
            "steps8\n",
            "d loss: 7.719097538938513e-08, g loss: 1.2836225323553663e-05\n",
            "\n",
            "epochs 114\n",
            "steps8\n",
            "d loss: 2.103246288243099e-07, g loss: 1.2160298865637742e-05\n",
            "\n",
            "epochs 115\n",
            "steps8\n",
            "d loss: -1.038453092405689e-07, g loss: 1.230178531841375e-05\n",
            "\n",
            "epochs 116\n",
            "steps8\n",
            "d loss: -1.2331054222158855e-07, g loss: 1.1089738109149039e-05\n",
            "\n",
            "epochs 117\n",
            "steps8\n",
            "d loss: 3.6032213301950833e-07, g loss: 1.1149710189783946e-05\n",
            "\n",
            "epochs 118\n",
            "steps8\n",
            "d loss: -2.5654571800259873e-09, g loss: 1.0926029062829912e-05\n",
            "\n",
            "epochs 119\n",
            "steps8\n",
            "d loss: -1.7699198906484526e-07, g loss: 1.2250534382474143e-05\n",
            "\n",
            "epochs 120\n",
            "steps8\n",
            "d loss: -5.592117986452649e-07, g loss: 1.3538418897951487e-05\n",
            "\n",
            "epochs 121\n",
            "steps8\n",
            "d loss: -3.4930064884974854e-07, g loss: 1.391707883158233e-05\n",
            "\n",
            "epochs 122\n",
            "steps8\n",
            "d loss: -1.4803288195253117e-07, g loss: 1.2002166840829886e-05\n",
            "\n",
            "epochs 123\n",
            "steps8\n",
            "d loss: 4.091069740752573e-07, g loss: 1.2525356396508869e-05\n",
            "\n",
            "epochs 124\n",
            "steps8\n",
            "d loss: -2.896213118219748e-07, g loss: 1.4561603165930137e-05\n",
            "\n",
            "epochs 125\n",
            "steps8\n",
            "d loss: 4.1709870401973603e-07, g loss: 1.662794966250658e-05\n",
            "\n",
            "epochs 126\n",
            "steps8\n",
            "d loss: 3.491220468276879e-07, g loss: 1.5971909306244925e-05\n",
            "\n",
            "epochs 127\n",
            "steps8\n",
            "d loss: 6.74549710311112e-07, g loss: 1.2747945220326073e-05\n",
            "\n",
            "epochs 128\n",
            "steps8\n",
            "d loss: 2.7309397410135716e-08, g loss: 1.2669969692069571e-05\n",
            "\n",
            "epochs 129\n",
            "steps8\n",
            "d loss: -2.4897394723666366e-07, g loss: 1.2303107723710127e-05\n",
            "\n",
            "epochs 130\n",
            "steps8\n",
            "d loss: 3.713046226039296e-08, g loss: 1.2727419743896462e-05\n",
            "\n",
            "epochs 131\n",
            "steps8\n",
            "d loss: -5.268395852908725e-08, g loss: 1.1892069778696168e-05\n",
            "\n",
            "epochs 132\n",
            "steps8\n",
            "d loss: -3.1960792057361687e-07, g loss: 1.0833826308953576e-05\n",
            "\n",
            "epochs 133\n",
            "steps8\n",
            "d loss: 1.2047257769154385e-07, g loss: 9.265901098842733e-06\n",
            "\n",
            "epochs 134\n",
            "steps8\n",
            "d loss: 3.4253673675266327e-07, g loss: 8.556992725061718e-06\n",
            "\n",
            "epochs 135\n",
            "steps8\n",
            "d loss: -2.9827208436472574e-07, g loss: 9.522480468149297e-06\n",
            "\n",
            "epochs 136\n",
            "steps8\n",
            "d loss: -2.4492362626915565e-07, g loss: 1.1161791917402297e-05\n",
            "\n",
            "epochs 137\n",
            "steps8\n",
            "d loss: -5.275933290249668e-08, g loss: 1.1812580851255916e-05\n",
            "\n",
            "epochs 138\n",
            "steps8\n",
            "d loss: 2.522324393794406e-07, g loss: 1.2882236660516355e-05\n",
            "\n",
            "epochs 139\n",
            "steps8\n",
            "d loss: -2.7706732907972764e-08, g loss: 1.3040307749179192e-05\n",
            "\n",
            "epochs 140\n",
            "steps8\n",
            "d loss: 3.791126346186502e-08, g loss: 1.5439101844094694e-05\n",
            "\n",
            "epochs 141\n",
            "steps8\n",
            "d loss: -1.9324181721458444e-07, g loss: 1.478929516451899e-05\n",
            "\n",
            "epochs 142\n",
            "steps8\n",
            "d loss: 1.2445514130376978e-07, g loss: 1.542120662634261e-05\n",
            "\n",
            "epochs 143\n",
            "steps8\n",
            "d loss: 1.536005811431096e-07, g loss: 1.4965489754104055e-05\n",
            "\n",
            "epochs 144\n",
            "steps8\n",
            "d loss: -9.502969078312162e-08, g loss: 1.2968040209671017e-05\n",
            "\n",
            "epochs 145\n",
            "steps8\n",
            "d loss: 2.9048976557533024e-07, g loss: 1.4031304999662098e-05\n",
            "\n",
            "epochs 146\n",
            "steps8\n",
            "d loss: 1.3035776191827608e-07, g loss: 1.66940844792407e-05\n",
            "\n",
            "epochs 147\n",
            "steps8\n",
            "d loss: -2.1668199678970268e-07, g loss: 1.5988072846084833e-05\n",
            "\n",
            "epochs 148\n",
            "steps8\n",
            "d loss: -2.9159423320379574e-08, g loss: 1.550840534036979e-05\n",
            "\n",
            "epochs 149\n",
            "steps8\n",
            "d loss: -1.3082512850814965e-08, g loss: 1.2287911886232905e-05\n",
            "\n",
            "epochs 150\n",
            "steps8\n",
            "d loss: 5.48416210222058e-08, g loss: 1.1092728527728468e-05\n",
            "\n",
            "epochs 151\n",
            "steps8\n",
            "d loss: 1.9129583961330354e-07, g loss: 1.0043697329820134e-05\n",
            "\n",
            "epochs 152\n",
            "steps8\n",
            "d loss: -6.133543593023205e-07, g loss: 1.0456086783960927e-05\n",
            "\n",
            "epochs 153\n",
            "steps8\n",
            "d loss: -5.498750397237018e-07, g loss: 9.967094229068607e-06\n",
            "\n",
            "epochs 154\n",
            "steps8\n",
            "d loss: 3.450387566772406e-07, g loss: 9.954254892363679e-06\n",
            "\n",
            "epochs 155\n",
            "steps8\n",
            "d loss: -6.18134663454839e-08, g loss: 9.938483344740234e-06\n",
            "\n",
            "epochs 156\n",
            "steps8\n",
            "d loss: 3.1838021641306113e-07, g loss: 8.819266440696083e-06\n",
            "\n",
            "epochs 157\n",
            "steps8\n",
            "d loss: -4.6451145863102283e-07, g loss: 9.736620995681733e-06\n",
            "\n",
            "epochs 158\n",
            "steps8\n",
            "d loss: 1.1178906333952909e-07, g loss: 9.056288035935722e-06\n",
            "\n",
            "epochs 159\n",
            "steps8\n",
            "d loss: 2.274335315632925e-07, g loss: 7.847171218600124e-06\n",
            "\n",
            "epochs 160\n",
            "steps8\n",
            "d loss: -3.6750674325958244e-07, g loss: 9.495027370576281e-06\n",
            "\n",
            "epochs 161\n",
            "steps8\n",
            "d loss: -7.120831924112281e-07, g loss: 1.166669335361803e-05\n",
            "\n",
            "epochs 162\n",
            "steps8\n",
            "d loss: -1.8869513951358385e-07, g loss: 1.1168923265358899e-05\n",
            "\n",
            "epochs 163\n",
            "steps8\n",
            "d loss: -1.732601049297955e-07, g loss: 1.059354872268159e-05\n",
            "\n",
            "epochs 164\n",
            "steps8\n",
            "d loss: -1.566463652125094e-07, g loss: 1.0040821507573128e-05\n",
            "\n",
            "epochs 165\n",
            "steps8\n",
            "d loss: 3.4345760013820836e-07, g loss: 9.98226460069418e-06\n",
            "\n",
            "epochs 166\n",
            "steps8\n",
            "d loss: -8.658184924570378e-08, g loss: 1.0555128937994596e-05\n",
            "\n",
            "epochs 167\n",
            "steps8\n",
            "d loss: -7.916219146864023e-08, g loss: 1.1543130312929861e-05\n",
            "\n",
            "epochs 168\n",
            "steps8\n",
            "d loss: -7.176276994869113e-07, g loss: 1.220117792399833e-05\n",
            "\n",
            "epochs 169\n",
            "steps8\n",
            "d loss: -4.382181941764429e-07, g loss: 1.297020116908243e-05\n",
            "\n",
            "epochs 170\n",
            "steps8\n",
            "d loss: -4.646943807529169e-07, g loss: 1.1401814845157787e-05\n",
            "\n",
            "epochs 171\n",
            "steps8\n",
            "d loss: -2.0995412342017516e-07, g loss: 9.96207381831482e-06\n",
            "\n",
            "epochs 172\n",
            "steps8\n",
            "d loss: 5.0982521315745544e-08, g loss: 1.0623763046169188e-05\n",
            "\n",
            "epochs 173\n",
            "steps8\n",
            "d loss: 3.1706497338745976e-07, g loss: 9.872570444713347e-06\n",
            "\n",
            "epochs 174\n",
            "steps8\n",
            "d loss: -2.9683752700293553e-07, g loss: 8.768529369262978e-06\n",
            "\n",
            "epochs 175\n",
            "steps8\n",
            "d loss: -7.709764986429946e-07, g loss: 7.011032721493393e-06\n",
            "\n",
            "epochs 176\n",
            "steps8\n",
            "d loss: -5.82208940613782e-07, g loss: 3.87686668545939e-06\n",
            "\n",
            "epochs 177\n",
            "steps8\n",
            "d loss: 1.9622319769041496e-07, g loss: 2.0671554921136703e-06\n",
            "\n",
            "epochs 178\n",
            "steps8\n",
            "d loss: 3.5531209618966386e-07, g loss: 2.5081812964344863e-06\n",
            "\n",
            "epochs 179\n",
            "steps8\n",
            "d loss: 1.5608048897775006e-07, g loss: 4.3425238800409716e-06\n",
            "\n",
            "epochs 180\n",
            "steps8\n",
            "d loss: 1.2957639228261542e-07, g loss: 4.004994934803108e-06\n",
            "\n",
            "epochs 181\n",
            "steps8\n",
            "d loss: 3.955764782404003e-08, g loss: 1.760376221682236e-06\n",
            "\n",
            "epochs 182\n",
            "steps8\n",
            "d loss: 3.003861763772875e-07, g loss: 2.795585487547214e-06\n",
            "\n",
            "epochs 183\n",
            "steps8\n",
            "d loss: -3.171390687839448e-07, g loss: 3.269748958700802e-06\n",
            "\n",
            "epochs 184\n",
            "steps8\n",
            "d loss: -9.305800574566092e-08, g loss: 1.0530557119636796e-06\n",
            "\n",
            "epochs 185\n",
            "steps8\n",
            "d loss: 1.4460673014582426e-07, g loss: 1.7200586626131553e-06\n",
            "\n",
            "epochs 186\n",
            "steps8\n",
            "d loss: 2.509157468466583e-07, g loss: 1.4116759530224954e-06\n",
            "\n",
            "epochs 187\n",
            "steps8\n",
            "d loss: 6.091595423640683e-07, g loss: 5.279683819026104e-07\n",
            "\n",
            "epochs 188\n",
            "steps8\n",
            "d loss: 2.881586169678485e-07, g loss: 2.343011544780893e-07\n",
            "\n",
            "epochs 189\n",
            "steps8\n",
            "d loss: -4.0314324678547564e-07, g loss: -2.023428578468156e-06\n",
            "\n",
            "epochs 190\n",
            "steps8\n",
            "d loss: -1.2181759245777357e-07, g loss: -2.1247828954074066e-06\n",
            "\n",
            "epochs 191\n",
            "steps8\n",
            "d loss: -4.4087940409553994e-08, g loss: -1.5366339312095079e-06\n",
            "\n",
            "epochs 192\n",
            "steps8\n",
            "d loss: -4.91947673708637e-08, g loss: -2.548145630498766e-06\n",
            "\n",
            "epochs 193\n",
            "steps8\n",
            "d loss: 5.0617558144949726e-08, g loss: -4.9345740080752876e-06\n",
            "\n",
            "epochs 194\n",
            "steps8\n",
            "d loss: -4.4558220224644174e-07, g loss: -6.4626033235981595e-06\n",
            "\n",
            "epochs 195\n",
            "steps8\n",
            "d loss: -3.7246991269057617e-08, g loss: -5.512754341907566e-06\n",
            "\n",
            "epochs 196\n",
            "steps8\n",
            "d loss: -1.847663924081644e-07, g loss: -2.9341144909267314e-06\n",
            "\n",
            "epochs 197\n",
            "steps8\n",
            "d loss: -2.9110429977663443e-07, g loss: 4.636046639916458e-07\n",
            "\n",
            "epochs 198\n",
            "steps8\n",
            "d loss: 2.219482269083528e-07, g loss: 1.869111883934238e-06\n",
            "\n",
            "epochs 199\n",
            "steps8\n",
            "d loss: -1.7056316892194445e-07, g loss: 1.9404469639994204e-06\n",
            "\n",
            "epochs 200\n",
            "steps8\n",
            "d loss: -5.905044986320718e-07, g loss: 1.8252121662953869e-06\n",
            "\n",
            "Saving checkpoint at epoch200\n",
            "\n",
            "\n",
            "\n",
            "epochs 201\n",
            "steps8\n",
            "d loss: 3.2051335097094125e-07, g loss: 2.8921397188241826e-07\n",
            "\n",
            "epochs 202\n",
            "steps8\n",
            "d loss: -1.8908303900389e-07, g loss: -2.5094672650993743e-07\n",
            "\n",
            "epochs 203\n",
            "steps8\n",
            "d loss: 4.764715413330123e-07, g loss: -1.7928574180814394e-08\n",
            "\n",
            "epochs 204\n",
            "steps8\n",
            "d loss: -4.867582674705773e-07, g loss: 4.86915610053984e-07\n",
            "\n",
            "epochs 205\n",
            "steps8\n",
            "d loss: 2.1745213985013834e-07, g loss: -1.0805548527059727e-06\n",
            "\n",
            "epochs 206\n",
            "steps8\n",
            "d loss: -5.824555842082191e-07, g loss: -2.3323011646425584e-06\n",
            "\n",
            "epochs 207\n",
            "steps8\n",
            "d loss: 2.344780227758747e-07, g loss: -3.563300197129138e-07\n",
            "\n",
            "epochs 208\n",
            "steps8\n",
            "d loss: -1.072735074103548e-07, g loss: -1.037886249832809e-06\n",
            "\n",
            "epochs 209\n",
            "steps8\n",
            "d loss: -3.5132620723743457e-07, g loss: -3.2116433885676088e-06\n",
            "\n",
            "epochs 210\n",
            "steps8\n",
            "d loss: 6.954947480153351e-07, g loss: -6.403279257938266e-06\n",
            "\n",
            "epochs 211\n",
            "steps8\n",
            "d loss: 6.03918920205615e-07, g loss: -8.29185955808498e-06\n",
            "\n",
            "epochs 212\n",
            "steps8\n",
            "d loss: -9.976491810448351e-08, g loss: -6.7935065999336075e-06\n",
            "\n",
            "epochs 213\n",
            "steps8\n",
            "d loss: 3.589637458389916e-07, g loss: -4.5313827286008745e-06\n",
            "\n",
            "epochs 214\n",
            "steps8\n",
            "d loss: -5.471598001349776e-07, g loss: -3.860000106215011e-06\n",
            "\n",
            "epochs 215\n",
            "steps8\n",
            "d loss: 2.5178133000736125e-07, g loss: -4.973833256372018e-06\n",
            "\n",
            "epochs 216\n",
            "steps8\n",
            "d loss: -3.340039143040485e-07, g loss: -4.887611339654541e-06\n",
            "\n",
            "epochs 217\n",
            "steps1"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1bd4e748835d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m            \u001b[0mn_critics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m            )\n\u001b[0;32m--> 214\u001b[0;31m   \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-1bd4e748835d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_critics\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_loss_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1bd4e748835d>\u001b[0m in \u001b[0;36mg_train\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mgradients_of_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    264\u001b[0m     factor = _safe_shape_div(\n\u001b[1;32m    265\u001b[0m         math_ops.reduce_prod(input_shape), math_ops.reduce_prod(output_shape))\n\u001b[0;32m--> 266\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtruediv\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m   \"\"\"\n\u001b[0;32m-> 1297\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1234\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mreal_div\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   7438\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   7439\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RealDiv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7440\u001b[0;31m         tld.op_callbacks, x, y)\n\u001b[0m\u001b[1;32m   7441\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7442\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}