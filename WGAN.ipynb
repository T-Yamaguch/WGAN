{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/WGAN/blob/master/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ea2b391-9f4b-482d-b0af-00e6edb4839c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = ReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = ReLU()\n",
        "    self.act2 = ReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class disc_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(disc_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = ReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    # x = self.norm(x) dにnorm入れないほうがいいという噂\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "class dense_block_wo_norm(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(dense_block_wo_norm, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.act = ReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "fa026e51-737f-42ce-dc9a-c9ba7c74f099"
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 256\n",
        "    self.layer_num = 5\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    while data_size*64 < final_size:\n",
        "      data_size *= 64\n",
        "      x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      # filter_num /= 2\n",
        "      x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 512)               6656      \n",
            "_________________________________________________________________\n",
            "dense_block_1 (dense_block)  (None, 4096)              2117632   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv_block (conv_block)      (None, 8, 8, 256)         1639680   \n",
            "_________________________________________________________________\n",
            "conv_block_1 (conv_block)    (None, 16, 16, 256)       1639680   \n",
            "_________________________________________________________________\n",
            "conv_block_2 (conv_block)    (None, 32, 32, 256)       1639680   \n",
            "_________________________________________________________________\n",
            "conv_block_3 (conv_block)    (None, 64, 64, 256)       1639680   \n",
            "_________________________________________________________________\n",
            "conv_block_4 (conv_block)    (None, 128, 128, 256)     1639680   \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 128, 128, 3)       19203     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 10,341,891\n",
            "Trainable params: 10,330,115\n",
            "Non-trainable params: 11,776\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "3cf5c4c6-9ed4-42b8-ca52-0d5de27b31cb"
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 8\n",
        "    self.layer_num = 5\n",
        "    self.latent_num = 8\n",
        "    self.input_shape = (128, 128, 3)\n",
        "    self.patch_shape = (32, 32, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.patch_inputs = Input(shape=self.patch_shape)\n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def patch_model(self):\n",
        "    x = self.patch_inputs\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      x = disc_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = dense_block(32, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    # x = Dense(self.latent_num)(x)\n",
        "    # x = Activation('sigmoid')(x)\n",
        "\n",
        "    outputs = x\n",
        "\n",
        "    return Model(inputs = self.patch_inputs, outputs = outputs, name = 'patch_model')\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    # h, w = self.input_shape[:-1]\n",
        "    # ph, pw = self.patch_shape[:-1]\n",
        "    # list_row_idx = [(i*ph, (i+1)*ph) for i in range(h//ph)]\n",
        "    # list_col_idx = [(i*pw, (i+1)*pw) for i in range(w//pw)]\n",
        "\n",
        "    # list_patch = []\n",
        "    # for row_idx in list_row_idx:\n",
        "    #     for col_idx in list_col_idx:\n",
        "    #         x_patch = Lambda(lambda z: z[:, row_idx[0]:row_idx[1], col_idx[0]:col_idx[1], :])(x)\n",
        "    #         list_patch.append(x_patch)\n",
        "\n",
        "    # patch_num = (h//ph)*(w//pw)\n",
        "\n",
        "    # patch_model = self.patch_model()\n",
        "    # x = [patch_model(list_patch[i]) for i in range(patch_num)]\n",
        "\n",
        "    # x = Concatenate(axis=-1)(x)\n",
        "\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      x = disc_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "    x = Conv2D(filter_num, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    # x = BatchNormalization(trainable=True)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # x1 = Dense(1)(x)\n",
        "    # x1 = Activation('tanh')(x1)\n",
        "\n",
        "    # x2 = Dense(self.latent_num)(x)\n",
        "    # x2 = Activation('sigmoid')(x2)\n",
        "\n",
        "    # outputs = [x1, x2]\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = dense_block_wo_norm(64, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "# d.patch_model().summary()\n",
        "d.model().summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "disc_block (disc_block)      (None, 64, 64, 8)         608       \n",
            "_________________________________________________________________\n",
            "disc_block_1 (disc_block)    (None, 32, 32, 16)        3216      \n",
            "_________________________________________________________________\n",
            "disc_block_2 (disc_block)    (None, 16, 16, 32)        12832     \n",
            "_________________________________________________________________\n",
            "disc_block_3 (disc_block)    (None, 8, 8, 64)          51264     \n",
            "_________________________________________________________________\n",
            "disc_block_4 (disc_block)    (None, 4, 4, 128)         204928    \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "re_lu_12 (ReLU)              (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_block_wo_norm (dense_b (None, 64)                262208    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,354,577\n",
            "Trainable params: 1,354,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ad134f8-4c0f-4bfa-86ac-2c19d8a3cd1e"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "class WGAN():\n",
        "  def __init__(self, \n",
        "               img_size=128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/WGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'drive/My Drive/samples/image'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image/255  # normalize to [0,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def discriminator_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    loss_d = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    return loss_d\n",
        "\n",
        "  def generator_loss(self, generated_outputs):\n",
        "    loss_g = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return loss_g\n",
        "\n",
        "  def mse_loss(self, true, pred):\n",
        "    loss =  tf.math.reduce_mean(MSE(true, pred))\n",
        "    return loss\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "  \n",
        "  def gan_train(self, imgs, n):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "\n",
        "      # ori_outputs, ori_styles = self.disc(imgs, training=True)\n",
        "      # gen_outputs, gen_styles = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      # gen_loss = self.generator_loss(gen_outputs)      \n",
        "      # disc_loss = self.discriminator_loss(ori_outputs, gen_outputs)\n",
        "\n",
        "      # re_gen_imgs = self.gen(ori_styles, training=True)\n",
        "\n",
        "      # endec_loss = self.mse_loss(imgs, re_gen_imgs)\n",
        "      # self.endec_history.append(endec_loss)\n",
        "      \n",
        "      disc_loss, gen_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_history.append(gen_loss)\n",
        "      self.d_history.append(disc_loss)\n",
        "\n",
        "      # endec_rate = 10*(0.98**n)\n",
        "      # g_loss = gen_loss + endec_loss * endec_rate\n",
        "      # d_loss = disc_loss + endec_loss * endec_rate\n",
        "\n",
        "      g_loss = gen_loss\n",
        "      d_loss = disc_loss\n",
        "\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array(gen_img*255, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/WGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=200)\n",
        "    # plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/WGAN/loss/gan_loss.png')\n",
        "    plt.close('all')\n",
        "    # plt.plot(self.endec_history)\n",
        "    # plt.savefig('drive/My Drive/WGAN/loss/endec_loss.png')\n",
        "    # plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    self.d_history.append(sum(self.d_temp)/len(self.d_temp))\n",
        "    self.g_history.append(sum(self.g_temp)/len(self.g_temp))\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "    [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.d_train(imgs)\n",
        "        [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "        if steps % self.n_critics == 0:\n",
        "          self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = WGAN(img_size = 128,\n",
        "           code_num = 8,\n",
        "           batch_size = 16,\n",
        "           train_epochs = 10000, \n",
        "           train_steps = 128, \n",
        "           checkpoint_epochs = 100, \n",
        "           image_epochs = 1, \n",
        "           start_epoch = 701,\n",
        "           optimizer = RMSprop(lr=1E-7),\n",
        "           n_critics = 128\n",
        "           )\n",
        "  a()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epochs 701\n",
            "steps128\n",
            "epochs 702\n",
            "steps128\n",
            "epochs 703\n",
            "steps128\n",
            "epochs 704\n",
            "steps128\n",
            "epochs 705\n",
            "steps128\n",
            "epochs 706\n",
            "steps128\n",
            "epochs 707\n",
            "steps128\n",
            "epochs 708\n",
            "steps128\n",
            "epochs 709\n",
            "steps128\n",
            "epochs 710\n",
            "steps128\n",
            "epochs 711\n",
            "steps128\n",
            "epochs 712\n",
            "steps128\n",
            "epochs 713\n",
            "steps128\n",
            "epochs 714\n",
            "steps128\n",
            "epochs 715\n",
            "steps128\n",
            "epochs 716\n",
            "steps128\n",
            "epochs 717\n",
            "steps128\n",
            "epochs 718\n",
            "steps128\n",
            "epochs 719\n",
            "steps128\n",
            "epochs 720\n",
            "steps128\n",
            "epochs 721\n",
            "steps128\n",
            "epochs 722\n",
            "steps128\n",
            "epochs 723\n",
            "steps128\n",
            "epochs 724\n",
            "steps128\n",
            "epochs 725\n",
            "steps128\n",
            "epochs 726\n",
            "steps128\n",
            "epochs 727\n",
            "steps128\n",
            "epochs 728\n",
            "steps128\n",
            "epochs 729\n",
            "steps128\n",
            "epochs 730\n",
            "steps128\n",
            "epochs 731\n",
            "steps128\n",
            "epochs 732\n",
            "steps128\n",
            "epochs 733\n",
            "steps128\n",
            "epochs 734\n",
            "steps128\n",
            "epochs 735\n",
            "steps128\n",
            "epochs 736\n",
            "steps128\n",
            "epochs 737\n",
            "steps128\n",
            "epochs 738\n",
            "steps128\n",
            "epochs 739\n",
            "steps128\n",
            "epochs 740\n",
            "steps128\n",
            "epochs 741\n",
            "steps128\n",
            "epochs 742\n",
            "steps128\n",
            "epochs 743\n",
            "steps128\n",
            "epochs 744\n",
            "steps128\n",
            "epochs 745\n",
            "steps128\n",
            "epochs 746\n",
            "steps128\n",
            "epochs 747\n",
            "steps128\n",
            "epochs 748\n",
            "steps128\n",
            "epochs 749\n",
            "steps128\n",
            "epochs 750\n",
            "steps128\n",
            "epochs 751\n",
            "steps128\n",
            "epochs 752\n",
            "steps128\n",
            "epochs 753\n",
            "steps128\n",
            "epochs 754\n",
            "steps128\n",
            "epochs 755\n",
            "steps128\n",
            "epochs 756\n",
            "steps128\n",
            "epochs 757\n",
            "steps128\n",
            "epochs 758\n",
            "steps128\n",
            "epochs 759\n",
            "steps128\n",
            "epochs 760\n",
            "steps128\n",
            "epochs 761\n",
            "steps128\n",
            "epochs 762\n",
            "steps128\n",
            "epochs 763\n",
            "steps128\n",
            "epochs 764\n",
            "steps128\n",
            "epochs 765\n",
            "steps128\n",
            "epochs 766\n",
            "steps128\n",
            "epochs 767\n",
            "steps128\n",
            "epochs 768\n",
            "steps128\n",
            "epochs 769\n",
            "steps128\n",
            "epochs 770\n",
            "steps128\n",
            "epochs 771\n",
            "steps128\n",
            "epochs 772\n",
            "steps128\n",
            "epochs 773\n",
            "steps128\n",
            "epochs 774\n",
            "steps128\n",
            "epochs 775\n",
            "steps128\n",
            "epochs 776\n",
            "steps128\n",
            "epochs 777\n",
            "steps128\n",
            "epochs 778\n",
            "steps128\n",
            "epochs 779\n",
            "steps128\n",
            "epochs 780\n",
            "steps128\n",
            "epochs 781\n",
            "steps128\n",
            "epochs 782\n",
            "steps128\n",
            "epochs 783\n",
            "steps128\n",
            "epochs 784\n",
            "steps128\n",
            "epochs 785\n",
            "steps128\n",
            "epochs 786\n",
            "steps128\n",
            "epochs 787\n",
            "steps128\n",
            "epochs 788\n",
            "steps128\n",
            "epochs 789\n",
            "steps128\n",
            "epochs 790\n",
            "steps128\n",
            "epochs 791\n",
            "steps128\n",
            "epochs 792\n",
            "steps128\n",
            "epochs 793\n",
            "steps128\n",
            "epochs 794\n",
            "steps128\n",
            "epochs 795\n",
            "steps128\n",
            "epochs 796\n",
            "steps128\n",
            "epochs 797\n",
            "steps128\n",
            "epochs 798\n",
            "steps128\n",
            "epochs 799\n",
            "steps128\n",
            "epochs 800\n",
            "steps128\n",
            "Saving checkpoint at epoch800\n",
            "\n",
            "\n",
            "\n",
            "epochs 801\n",
            "steps128\n",
            "epochs 802\n",
            "steps128\n",
            "epochs 803\n",
            "steps128\n",
            "epochs 804\n",
            "steps128\n",
            "epochs 805\n",
            "steps128\n",
            "epochs 806\n",
            "steps128\n",
            "epochs 807\n",
            "steps128\n",
            "epochs 808\n",
            "steps128\n",
            "epochs 809\n",
            "steps128\n",
            "epochs 810\n",
            "steps128\n",
            "epochs 811\n",
            "steps128\n",
            "epochs 812\n",
            "steps128\n",
            "epochs 813\n",
            "steps128\n",
            "epochs 814\n",
            "steps128\n",
            "epochs 815\n",
            "steps128\n",
            "epochs 816\n",
            "steps128\n",
            "epochs 817\n",
            "steps128\n",
            "epochs 818\n",
            "steps128\n",
            "epochs 819\n",
            "steps128\n",
            "epochs 820\n",
            "steps128\n",
            "epochs 821\n",
            "steps128\n",
            "epochs 822\n",
            "steps128\n",
            "epochs 823\n",
            "steps128\n",
            "epochs 824\n",
            "steps128\n",
            "epochs 825\n",
            "steps128\n",
            "epochs 826\n",
            "steps128\n",
            "epochs 827\n",
            "steps128\n",
            "epochs 828\n",
            "steps128\n",
            "epochs 829\n",
            "steps128\n",
            "epochs 830\n",
            "steps128\n",
            "epochs 831\n",
            "steps128\n",
            "epochs 832\n",
            "steps128\n",
            "epochs 833\n",
            "steps128\n",
            "epochs 834\n",
            "steps128\n",
            "epochs 835\n",
            "steps128\n",
            "epochs 836\n",
            "steps128\n",
            "epochs 837\n",
            "steps128\n",
            "epochs 838\n",
            "steps128\n",
            "epochs 839\n",
            "steps128\n",
            "epochs 840\n",
            "steps128\n",
            "epochs 841\n",
            "steps128\n",
            "epochs 842\n",
            "steps128\n",
            "epochs 843\n",
            "steps128\n",
            "epochs 844\n",
            "steps128\n",
            "epochs 845\n",
            "steps128\n",
            "epochs 846\n",
            "steps128\n",
            "epochs 847\n",
            "steps128\n",
            "epochs 848\n",
            "steps128\n",
            "epochs 849\n",
            "steps128\n",
            "epochs 850\n",
            "steps128\n",
            "epochs 851\n",
            "steps128\n",
            "epochs 852\n",
            "steps128\n",
            "epochs 853\n",
            "steps128\n",
            "epochs 854\n",
            "steps128\n",
            "epochs 855\n",
            "steps128\n",
            "epochs 856\n",
            "steps128\n",
            "epochs 857\n",
            "steps128\n",
            "epochs 858\n",
            "steps128\n",
            "epochs 859\n",
            "steps128\n",
            "epochs 860\n",
            "steps128\n",
            "epochs 861\n",
            "steps128\n",
            "epochs 862\n",
            "steps128\n",
            "epochs 863\n",
            "steps128\n",
            "epochs 864\n",
            "steps128\n",
            "epochs 865\n",
            "steps128\n",
            "epochs 866\n",
            "steps128\n",
            "epochs 867\n",
            "steps128\n",
            "epochs 868\n",
            "steps128\n",
            "epochs 869\n",
            "steps128\n",
            "epochs 870\n",
            "steps128\n",
            "epochs 871\n",
            "steps128\n",
            "epochs 872\n",
            "steps128\n",
            "epochs 873\n",
            "steps128\n",
            "epochs 874\n",
            "steps128\n",
            "epochs 875\n",
            "steps128\n",
            "epochs 876\n",
            "steps128\n",
            "epochs 877\n",
            "steps128\n",
            "epochs 878\n",
            "steps128\n",
            "epochs 879\n",
            "steps128\n",
            "epochs 880\n",
            "steps128\n",
            "epochs 881\n",
            "steps128\n",
            "epochs 882\n",
            "steps128\n",
            "epochs 883\n",
            "steps128\n",
            "epochs 884\n",
            "steps128\n",
            "epochs 885\n",
            "steps128\n",
            "epochs 886\n",
            "steps128\n",
            "epochs 887\n",
            "steps128\n",
            "epochs 888\n",
            "steps128\n",
            "epochs 889\n",
            "steps128\n",
            "epochs 890\n",
            "steps128\n",
            "epochs 891\n",
            "steps128\n",
            "epochs 892\n",
            "steps128\n",
            "epochs 893\n",
            "steps128\n",
            "epochs 894\n",
            "steps128\n",
            "epochs 895\n",
            "steps128\n",
            "epochs 896\n",
            "steps128\n",
            "epochs 897\n",
            "steps128\n",
            "epochs 898\n",
            "steps128\n",
            "epochs 899\n",
            "steps128\n",
            "epochs 900\n",
            "steps128\n",
            "Saving checkpoint at epoch900\n",
            "\n",
            "\n",
            "\n",
            "epochs 901\n",
            "steps128\n",
            "epochs 902\n",
            "steps128\n",
            "epochs 903\n",
            "steps128\n",
            "epochs 904\n",
            "steps128\n",
            "epochs 905\n",
            "steps128\n",
            "epochs 906\n",
            "steps128\n",
            "epochs 907\n",
            "steps128\n",
            "epochs 908\n",
            "steps128\n",
            "epochs 909\n",
            "steps128\n",
            "epochs 910\n",
            "steps128\n",
            "epochs 911\n",
            "steps128\n",
            "epochs 912\n",
            "steps128\n",
            "epochs 913\n",
            "steps128\n",
            "epochs 914\n",
            "steps128\n",
            "epochs 915\n",
            "steps128\n",
            "epochs 916\n",
            "steps128\n",
            "epochs 917\n",
            "steps128\n",
            "epochs 918\n",
            "steps128\n",
            "epochs 919\n",
            "steps128\n",
            "epochs 920\n",
            "steps128\n",
            "epochs 921\n",
            "steps128\n",
            "epochs 922\n",
            "steps128\n",
            "epochs 923\n",
            "steps128\n",
            "epochs 924\n",
            "steps128\n",
            "epochs 925\n",
            "steps128\n",
            "epochs 926\n",
            "steps128\n",
            "epochs 927\n",
            "steps128\n",
            "epochs 928\n",
            "steps128\n",
            "epochs 929\n",
            "steps128\n",
            "epochs 930\n",
            "steps128\n",
            "epochs 931\n",
            "steps128\n",
            "epochs 932\n",
            "steps128\n",
            "epochs 933\n",
            "steps128\n",
            "epochs 934\n",
            "steps128\n",
            "epochs 935\n",
            "steps128\n",
            "epochs 936\n",
            "steps128\n",
            "epochs 937\n",
            "steps128\n",
            "epochs 938\n",
            "steps128\n",
            "epochs 939\n",
            "steps128\n",
            "epochs 940\n",
            "steps128\n",
            "epochs 941\n",
            "steps128\n",
            "epochs 942\n",
            "steps128\n",
            "epochs 943\n",
            "steps128\n",
            "epochs 944\n",
            "steps128\n",
            "epochs 945\n",
            "steps128\n",
            "epochs 946\n",
            "steps128\n",
            "epochs 947\n",
            "steps128\n",
            "epochs 948\n",
            "steps128\n",
            "epochs 949\n",
            "steps128\n",
            "epochs 950\n",
            "steps128\n",
            "epochs 951\n",
            "steps128\n",
            "epochs 952\n",
            "steps128\n",
            "epochs 953\n",
            "steps128\n",
            "epochs 954\n",
            "steps128\n",
            "epochs 955\n",
            "steps128\n",
            "epochs 956\n",
            "steps128\n",
            "epochs 957\n",
            "steps128\n",
            "epochs 958\n",
            "steps128\n",
            "epochs 959\n",
            "steps128\n",
            "epochs 960\n",
            "steps128\n",
            "epochs 961\n",
            "steps128\n",
            "epochs 962\n",
            "steps128\n",
            "epochs 963\n",
            "steps128\n",
            "epochs 964\n",
            "steps128\n",
            "epochs 965\n",
            "steps128\n",
            "epochs 966\n",
            "steps128\n",
            "epochs 967\n",
            "steps128\n",
            "epochs 968\n",
            "steps128\n",
            "epochs 969\n",
            "steps128\n",
            "epochs 970\n",
            "steps128\n",
            "epochs 971\n",
            "steps128\n",
            "epochs 972\n",
            "steps128\n",
            "epochs 973\n",
            "steps128\n",
            "epochs 974\n",
            "steps128\n",
            "epochs 975\n",
            "steps128\n",
            "epochs 976\n",
            "steps128\n",
            "epochs 977\n",
            "steps128\n",
            "epochs 978\n",
            "steps128\n",
            "epochs 979\n",
            "steps128\n",
            "epochs 980\n",
            "steps128\n",
            "epochs 981\n",
            "steps128\n",
            "epochs 982\n",
            "steps128\n",
            "epochs 983\n",
            "steps128\n",
            "epochs 984\n",
            "steps128\n",
            "epochs 985\n",
            "steps128\n",
            "epochs 986\n",
            "steps128\n",
            "epochs 987\n",
            "steps128\n",
            "epochs 988\n",
            "steps128\n",
            "epochs 989\n",
            "steps128\n",
            "epochs 990\n",
            "steps128\n",
            "epochs 991\n",
            "steps128\n",
            "epochs 992\n",
            "steps128\n",
            "epochs 993\n",
            "steps128\n",
            "epochs 994\n",
            "steps128\n",
            "epochs 995\n",
            "steps128\n",
            "epochs 996\n",
            "steps128\n",
            "epochs 997\n",
            "steps128\n",
            "epochs 998\n",
            "steps128\n",
            "epochs 999\n",
            "steps128\n",
            "epochs 1000\n",
            "steps128\n",
            "Saving checkpoint at epoch1000\n",
            "\n",
            "\n",
            "\n",
            "epochs 1001\n",
            "steps128\n",
            "epochs 1002\n",
            "steps128\n",
            "epochs 1003\n",
            "steps128\n",
            "epochs 1004\n",
            "steps128\n",
            "epochs 1005\n",
            "steps128\n",
            "epochs 1006\n",
            "steps128\n",
            "epochs 1007\n",
            "steps128\n",
            "epochs 1008\n",
            "steps128\n",
            "epochs 1009\n",
            "steps128\n",
            "epochs 1010\n",
            "steps128\n",
            "epochs 1011\n",
            "steps128\n",
            "epochs 1012\n",
            "steps128\n",
            "epochs 1013\n",
            "steps128\n",
            "epochs 1014\n",
            "steps128\n",
            "epochs 1015\n",
            "steps128\n",
            "epochs 1016\n",
            "steps128\n",
            "epochs 1017\n",
            "steps128\n",
            "epochs 1018\n",
            "steps128\n",
            "epochs 1019\n",
            "steps128\n",
            "epochs 1020\n",
            "steps128\n",
            "epochs 1021\n",
            "steps128\n",
            "epochs 1022\n",
            "steps128\n",
            "epochs 1023\n",
            "steps128\n",
            "epochs 1024\n",
            "steps128\n",
            "epochs 1025\n",
            "steps128\n",
            "epochs 1026\n",
            "steps128\n",
            "epochs 1027\n",
            "steps128\n",
            "epochs 1028\n",
            "steps128\n",
            "epochs 1029\n",
            "steps128\n",
            "epochs 1030\n",
            "steps128\n",
            "epochs 1031\n",
            "steps128\n",
            "epochs 1032\n",
            "steps128\n",
            "epochs 1033\n",
            "steps128\n",
            "epochs 1034\n",
            "steps128\n",
            "epochs 1035\n",
            "steps128\n",
            "epochs 1036\n",
            "steps128\n",
            "epochs 1037\n",
            "steps128\n",
            "epochs 1038\n",
            "steps128\n",
            "epochs 1039\n",
            "steps128\n",
            "epochs 1040\n",
            "steps128\n",
            "epochs 1041\n",
            "steps128\n",
            "epochs 1042\n",
            "steps128\n",
            "epochs 1043\n",
            "steps128\n",
            "epochs 1044\n",
            "steps128\n",
            "epochs 1045\n",
            "steps128\n",
            "epochs 1046\n",
            "steps128\n",
            "epochs 1047\n",
            "steps128\n",
            "epochs 1048\n",
            "steps128\n",
            "epochs 1049\n",
            "steps128\n",
            "epochs 1050\n",
            "steps128\n",
            "epochs 1051\n",
            "steps128\n",
            "epochs 1052\n",
            "steps128\n",
            "epochs 1053\n",
            "steps128\n",
            "epochs 1054\n",
            "steps128\n",
            "epochs 1055\n",
            "steps128\n",
            "epochs 1056\n",
            "steps128\n",
            "epochs 1057\n",
            "steps128\n",
            "epochs 1058\n",
            "steps128\n",
            "epochs 1059\n",
            "steps128\n",
            "epochs 1060\n",
            "steps128\n",
            "epochs 1061\n",
            "steps128\n",
            "epochs 1062\n",
            "steps128\n",
            "epochs 1063\n",
            "steps128\n",
            "epochs 1064\n",
            "steps128\n",
            "epochs 1065\n",
            "steps128\n",
            "epochs 1066\n",
            "steps128\n",
            "epochs 1067\n",
            "steps128\n",
            "epochs 1068\n",
            "steps128\n",
            "epochs 1069\n",
            "steps128\n",
            "epochs 1070\n",
            "steps128\n",
            "epochs 1071\n",
            "steps128\n",
            "epochs 1072\n",
            "steps128\n",
            "epochs 1073\n",
            "steps128\n",
            "epochs 1074\n",
            "steps128\n",
            "epochs 1075\n",
            "steps128\n",
            "epochs 1076\n",
            "steps128\n",
            "epochs 1077\n",
            "steps128\n",
            "epochs 1078\n",
            "steps128\n",
            "epochs 1079\n",
            "steps128\n",
            "epochs 1080\n",
            "steps128\n",
            "epochs 1081\n",
            "steps128\n",
            "epochs 1082\n",
            "steps128\n",
            "epochs 1083\n",
            "steps128\n",
            "epochs 1084\n",
            "steps128\n",
            "epochs 1085\n",
            "steps128\n",
            "epochs 1086\n",
            "steps128\n",
            "epochs 1087\n",
            "steps128\n",
            "epochs 1088\n",
            "steps128\n",
            "epochs 1089\n",
            "steps128\n",
            "epochs 1090\n",
            "steps128\n",
            "epochs 1091\n",
            "steps128\n",
            "epochs 1092\n",
            "steps128\n",
            "epochs 1093\n",
            "steps128\n",
            "epochs 1094\n",
            "steps128\n",
            "epochs 1095\n",
            "steps128\n",
            "epochs 1096\n",
            "steps128\n",
            "epochs 1097\n",
            "steps128\n",
            "epochs 1098\n",
            "steps128\n",
            "epochs 1099\n",
            "steps128\n",
            "epochs 1100\n",
            "steps128\n",
            "Saving checkpoint at epoch1100\n",
            "\n",
            "\n",
            "\n",
            "epochs 1101\n",
            "steps128\n",
            "epochs 1102\n",
            "steps128\n",
            "epochs 1103\n",
            "steps128\n",
            "epochs 1104\n",
            "steps128\n",
            "epochs 1105\n",
            "steps128\n",
            "epochs 1106\n",
            "steps128\n",
            "epochs 1107\n",
            "steps128\n",
            "epochs 1108\n",
            "steps128\n",
            "epochs 1109\n",
            "steps128\n",
            "epochs 1110\n",
            "steps128\n",
            "epochs 1111\n",
            "steps128\n",
            "epochs 1112\n",
            "steps128\n",
            "epochs 1113\n",
            "steps128\n",
            "epochs 1114\n",
            "steps128\n",
            "epochs 1115\n",
            "steps128\n",
            "epochs 1116\n",
            "steps128\n",
            "epochs 1117\n",
            "steps128\n",
            "epochs 1118\n",
            "steps128\n",
            "epochs 1119\n",
            "steps128\n",
            "epochs 1120\n",
            "steps128\n",
            "epochs 1121\n",
            "steps128\n",
            "epochs 1122\n",
            "steps128\n",
            "epochs 1123\n",
            "steps128\n",
            "epochs 1124\n",
            "steps128\n",
            "epochs 1125\n",
            "steps128\n",
            "epochs 1126\n",
            "steps128\n",
            "epochs 1127\n",
            "steps128\n",
            "epochs 1128\n",
            "steps128\n",
            "epochs 1129\n",
            "steps128\n",
            "epochs 1130\n",
            "steps128\n",
            "epochs 1131\n",
            "steps128\n",
            "epochs 1132\n",
            "steps128\n",
            "epochs 1133\n",
            "steps128\n",
            "epochs 1134\n",
            "steps128\n",
            "epochs 1135\n",
            "steps128\n",
            "epochs 1136\n",
            "steps128\n",
            "epochs 1137\n",
            "steps128\n",
            "epochs 1138\n",
            "steps128\n",
            "epochs 1139\n",
            "steps128\n",
            "epochs 1140\n",
            "steps128\n",
            "epochs 1141\n",
            "steps128\n",
            "epochs 1142\n",
            "steps128\n",
            "epochs 1143\n",
            "steps128\n",
            "epochs 1144\n",
            "steps128\n",
            "epochs 1145\n",
            "steps128\n",
            "epochs 1146\n",
            "steps128\n",
            "epochs 1147\n",
            "steps128\n",
            "epochs 1148\n",
            "steps128\n",
            "epochs 1149\n",
            "steps128\n",
            "epochs 1150\n",
            "steps128\n",
            "epochs 1151\n",
            "steps128\n",
            "epochs 1152\n",
            "steps128\n",
            "epochs 1153\n",
            "steps128\n",
            "epochs 1154\n",
            "steps128\n",
            "epochs 1155\n",
            "steps128\n",
            "epochs 1156\n",
            "steps128\n",
            "epochs 1157\n",
            "steps128\n",
            "epochs 1158\n",
            "steps128\n",
            "epochs 1159\n",
            "steps128\n",
            "epochs 1160\n",
            "steps128\n",
            "epochs 1161\n",
            "steps128\n",
            "epochs 1162\n",
            "steps128\n",
            "epochs 1163\n",
            "steps128\n",
            "epochs 1164\n",
            "steps128\n",
            "epochs 1165\n",
            "steps128\n",
            "epochs 1166\n",
            "steps128\n",
            "epochs 1167\n",
            "steps128\n",
            "epochs 1168\n",
            "steps128\n",
            "epochs 1169\n",
            "steps128\n",
            "epochs 1170\n",
            "steps128\n",
            "epochs 1171\n",
            "steps128\n",
            "epochs 1172\n",
            "steps128\n",
            "epochs 1173\n",
            "steps128\n",
            "epochs 1174\n",
            "steps128\n",
            "epochs 1175\n",
            "steps128\n",
            "epochs 1176\n",
            "steps128\n",
            "epochs 1177\n",
            "steps128\n",
            "epochs 1178\n",
            "steps128\n",
            "epochs 1179\n",
            "steps128\n",
            "epochs 1180\n",
            "steps128\n",
            "epochs 1181\n",
            "steps128\n",
            "epochs 1182\n",
            "steps128\n",
            "epochs 1183\n",
            "steps128\n",
            "epochs 1184\n",
            "steps128\n",
            "epochs 1185\n",
            "steps128\n",
            "epochs 1186\n",
            "steps128\n",
            "epochs 1187\n",
            "steps128\n",
            "epochs 1188\n",
            "steps128\n",
            "epochs 1189\n",
            "steps128\n",
            "epochs 1190\n",
            "steps128\n",
            "epochs 1191\n",
            "steps128\n",
            "epochs 1192\n",
            "steps128\n",
            "epochs 1193\n",
            "steps128\n",
            "epochs 1194\n",
            "steps128\n",
            "epochs 1195\n",
            "steps128\n",
            "epochs 1196\n",
            "steps128\n",
            "epochs 1197\n",
            "steps128\n",
            "epochs 1198\n",
            "steps128\n",
            "epochs 1199\n",
            "steps128\n",
            "epochs 1200\n",
            "steps128\n",
            "Saving checkpoint at epoch1200\n",
            "\n",
            "\n",
            "\n",
            "epochs 1201\n",
            "steps128\n",
            "epochs 1202\n",
            "steps128\n",
            "epochs 1203\n",
            "steps128\n",
            "epochs 1204\n",
            "steps128\n",
            "epochs 1205\n",
            "steps128\n",
            "epochs 1206\n",
            "steps128\n",
            "epochs 1207\n",
            "steps128\n",
            "epochs 1208\n",
            "steps128\n",
            "epochs 1209\n",
            "steps128\n",
            "epochs 1210\n",
            "steps128\n",
            "epochs 1211\n",
            "steps128\n",
            "epochs 1212\n",
            "steps128\n",
            "epochs 1213\n",
            "steps128\n",
            "epochs 1214\n",
            "steps128\n",
            "epochs 1215\n",
            "steps128\n",
            "epochs 1216\n",
            "steps128\n",
            "epochs 1217\n",
            "steps128\n",
            "epochs 1218\n",
            "steps128\n",
            "epochs 1219\n",
            "steps128\n",
            "epochs 1220\n",
            "steps128\n",
            "epochs 1221\n",
            "steps128\n",
            "epochs 1222\n",
            "steps128\n",
            "epochs 1223\n",
            "steps128\n",
            "epochs 1224\n",
            "steps128\n",
            "epochs 1225\n",
            "steps128\n",
            "epochs 1226\n",
            "steps128\n",
            "epochs 1227\n",
            "steps128\n",
            "epochs 1228\n",
            "steps128\n",
            "epochs 1229\n",
            "steps128\n",
            "epochs 1230\n",
            "steps128\n",
            "epochs 1231\n",
            "steps128\n",
            "epochs 1232\n",
            "steps128\n",
            "epochs 1233\n",
            "steps128\n",
            "epochs 1234\n",
            "steps128\n",
            "epochs 1235\n",
            "steps128\n",
            "epochs 1236\n",
            "steps128\n",
            "epochs 1237\n",
            "steps128\n",
            "epochs 1238\n",
            "steps128\n",
            "epochs 1239\n",
            "steps128\n",
            "epochs 1240\n",
            "steps128\n",
            "epochs 1241\n",
            "steps128\n",
            "epochs 1242\n",
            "steps128\n",
            "epochs 1243\n",
            "steps128\n",
            "epochs 1244\n",
            "steps128\n",
            "epochs 1245\n",
            "steps128\n",
            "epochs 1246\n",
            "steps128\n",
            "epochs 1247\n",
            "steps128\n",
            "epochs 1248\n",
            "steps128\n",
            "epochs 1249\n",
            "steps128\n",
            "epochs 1250\n",
            "steps128\n",
            "epochs 1251\n",
            "steps128\n",
            "epochs 1252\n",
            "steps128\n",
            "epochs 1253\n",
            "steps128\n",
            "epochs 1254\n",
            "steps128\n",
            "epochs 1255\n",
            "steps128\n",
            "epochs 1256\n",
            "steps128\n",
            "epochs 1257\n",
            "steps128\n",
            "epochs 1258\n",
            "steps128\n",
            "epochs 1259\n",
            "steps128\n",
            "epochs 1260\n",
            "steps128\n",
            "epochs 1261\n",
            "steps128\n",
            "epochs 1262\n",
            "steps128\n",
            "epochs 1263\n",
            "steps128\n",
            "epochs 1264\n",
            "steps128\n",
            "epochs 1265\n",
            "steps128\n",
            "epochs 1266\n",
            "steps128\n",
            "epochs 1267\n",
            "steps128\n",
            "epochs 1268\n",
            "steps128\n",
            "epochs 1269\n",
            "steps128\n",
            "epochs 1270\n",
            "steps128\n",
            "epochs 1271\n",
            "steps128\n",
            "epochs 1272\n",
            "steps128\n",
            "epochs 1273\n",
            "steps128\n",
            "epochs 1274\n",
            "steps128\n",
            "epochs 1275\n",
            "steps128\n",
            "epochs 1276\n",
            "steps128\n",
            "epochs 1277\n",
            "steps128\n",
            "epochs 1278\n",
            "steps128\n",
            "epochs 1279\n",
            "steps128\n",
            "epochs 1280\n",
            "steps128\n",
            "epochs 1281\n",
            "steps128\n",
            "epochs 1282\n",
            "steps128\n",
            "epochs 1283\n",
            "steps128\n",
            "epochs 1284\n",
            "steps128\n",
            "epochs 1285\n",
            "steps128\n",
            "epochs 1286\n",
            "steps128\n",
            "epochs 1287\n",
            "steps128\n",
            "epochs 1288\n",
            "steps128\n",
            "epochs 1289\n",
            "steps128\n",
            "epochs 1290\n",
            "steps128\n",
            "epochs 1291\n",
            "steps128\n",
            "epochs 1292\n",
            "steps128\n",
            "epochs 1293\n",
            "steps128\n",
            "epochs 1294\n",
            "steps128\n",
            "epochs 1295\n",
            "steps128\n",
            "epochs 1296\n",
            "steps128\n",
            "epochs 1297\n",
            "steps128\n",
            "epochs 1298\n",
            "steps128\n",
            "epochs 1299\n",
            "steps128\n",
            "epochs 1300\n",
            "steps128\n",
            "Saving checkpoint at epoch1300\n",
            "\n",
            "\n",
            "\n",
            "epochs 1301\n",
            "steps128\n",
            "epochs 1302\n",
            "steps128\n",
            "epochs 1303\n",
            "steps128\n",
            "epochs 1304\n",
            "steps128\n",
            "epochs 1305\n",
            "steps128\n",
            "epochs 1306\n",
            "steps128\n",
            "epochs 1307\n",
            "steps128\n",
            "epochs 1308\n",
            "steps128\n",
            "epochs 1309\n",
            "steps128\n",
            "epochs 1310\n",
            "steps128\n",
            "epochs 1311\n",
            "steps128\n",
            "epochs 1312\n",
            "steps128\n",
            "epochs 1313\n",
            "steps128\n",
            "epochs 1314\n",
            "steps128\n",
            "epochs 1315\n",
            "steps128\n",
            "epochs 1316\n",
            "steps128\n",
            "epochs 1317\n",
            "steps128\n",
            "epochs 1318\n",
            "steps128\n",
            "epochs 1319\n",
            "steps128\n",
            "epochs 1320\n",
            "steps128\n",
            "epochs 1321\n",
            "steps128\n",
            "epochs 1322\n",
            "steps128\n",
            "epochs 1323\n",
            "steps128\n",
            "epochs 1324\n",
            "steps128\n",
            "epochs 1325\n",
            "steps128\n",
            "epochs 1326\n",
            "steps128\n",
            "epochs 1327\n",
            "steps128\n",
            "epochs 1328\n",
            "steps128\n",
            "epochs 1329\n",
            "steps128\n",
            "epochs 1330\n",
            "steps128\n",
            "epochs 1331\n",
            "steps128\n",
            "epochs 1332\n",
            "steps128\n",
            "epochs 1333\n",
            "steps128\n",
            "epochs 1334\n",
            "steps128\n",
            "epochs 1335\n",
            "steps128\n",
            "epochs 1336\n",
            "steps128\n",
            "epochs 1337\n",
            "steps128\n",
            "epochs 1338\n",
            "steps128\n",
            "epochs 1339\n",
            "steps128\n",
            "epochs 1340\n",
            "steps128\n",
            "epochs 1341\n",
            "steps128\n",
            "epochs 1342\n",
            "steps128\n",
            "epochs 1343\n",
            "steps128\n",
            "epochs 1344\n",
            "steps128\n",
            "epochs 1345\n",
            "steps16Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}