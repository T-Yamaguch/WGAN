{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/WGAN/blob/master/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "outputId": "6463e739-1a70-4f0f-a74c-b1a45d4b8811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCaY7uIeBtcK",
        "outputId": "db52e38d-5243-461a-80d1-6218a72e201f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 27 17:53:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class up_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(up_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = LeakyReLU()\n",
        "    self.act2 = LeakyReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class down_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(down_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    # x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.drop = Dropout(0.3)\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "outputId": "e022b612-eb1f-42c6-c0f8-6b0dc508d245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 1024\n",
        "    self.layer_num = 3\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    # while data_size*64 < final_size:\n",
        "    #   data_size *= 64\n",
        "    #   x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num /= 2\n",
        "      x = up_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 16384)             212992    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "up_block (up_block)          (None, 8, 8, 512)         13109760  \n",
            "_________________________________________________________________\n",
            "up_block_1 (up_block)        (None, 16, 16, 256)       3278080   \n",
            "_________________________________________________________________\n",
            "up_block_2 (up_block)        (None, 32, 32, 128)       819840    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 3)         9603      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 17,430,275\n",
            "Trainable params: 17,395,715\n",
            "Non-trainable params: 34,560\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "outputId": "b4f912ba-4a5d-43d6-ea0e-8b4d84d6aad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 128\n",
        "    self.layer_num = 3\n",
        "    self.input_shape = (32, 32, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.kernel_size = (4, 4)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer, batch_norm = False)(x)\n",
        "      x = down_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "    x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer, batch_norm = False)(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = dense_block(1, kernel_regularizer= self.kernel_regularizer, batch_norm = False)(x)\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "d.model().summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "down_block (down_block)      (None, 16, 16, 128)       6272      \n",
            "_________________________________________________________________\n",
            "down_block_1 (down_block)    (None, 8, 8, 256)         524544    \n",
            "_________________________________________________________________\n",
            "down_block_2 (down_block)    (None, 4, 4, 512)         2097664   \n",
            "_________________________________________________________________\n",
            "conv_block (conv_block)      (None, 4, 4, 1024)        8389632   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_block_1 (dense_block)  (None, 1)                 16385     \n",
            "=================================================================\n",
            "Total params: 11,034,497\n",
            "Trainable params: 11,034,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj",
        "outputId": "a225042f-d00c-419b-80ae-1b588498d019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "class WGAN():\n",
        "  def __init__(self, \n",
        "               img_size=128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/WGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'drive/My Drive/samples/image'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image/255  # normalize to [0,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def discriminator_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    loss_d = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    return loss_d\n",
        "\n",
        "  def generator_loss(self, generated_outputs):\n",
        "    loss_g = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return loss_g\n",
        "\n",
        "  def mse_loss(self, true, pred):\n",
        "    loss =  tf.math.reduce_mean(MSE(true, pred))\n",
        "    return loss\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.wasserstein_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array(gen_img*255, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/WGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=200)\n",
        "    # plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/WGAN/loss/gan_loss.png')\n",
        "    plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    self.d_history.append(sum(self.d_temp)/len(self.d_temp))\n",
        "    self.g_history.append(sum(self.g_temp)/len(self.g_temp))\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "    [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.d_train(imgs)\n",
        "        [w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in self.disc.variables]\n",
        "\n",
        "        if steps % self.n_critics == 0:\n",
        "          self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = WGAN(img_size = 32,\n",
        "           code_num = 8,\n",
        "           batch_size = 256,\n",
        "           train_epochs = 10000, \n",
        "           train_steps = 8, \n",
        "           checkpoint_epochs = 100, \n",
        "           image_epochs = 10, \n",
        "           start_epoch = 1,\n",
        "           optimizer = RMSprop(lr=1E-6),\n",
        "           n_critics = 1\n",
        "           )\n",
        "  a()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epochs 1\n",
            "steps8\n",
            "epochs 2\n",
            "steps8\n",
            "epochs 3\n",
            "steps8\n",
            "epochs 4\n",
            "steps8\n",
            "epochs 5\n",
            "steps8\n",
            "epochs 6\n",
            "steps8\n",
            "epochs 7\n",
            "steps8\n",
            "epochs 8\n",
            "steps8\n",
            "epochs 9\n",
            "steps8\n",
            "epochs 10\n",
            "steps8\n",
            "epochs 11\n",
            "steps8\n",
            "epochs 12\n",
            "steps8\n",
            "epochs 13\n",
            "steps8\n",
            "epochs 14\n",
            "steps8\n",
            "epochs 15\n",
            "steps8\n",
            "epochs 16\n",
            "steps8\n",
            "epochs 17\n",
            "steps8\n",
            "epochs 18\n",
            "steps8\n",
            "epochs 19\n",
            "steps8\n",
            "epochs 20\n",
            "steps8\n",
            "epochs 21\n",
            "steps8\n",
            "epochs 22\n",
            "steps8\n",
            "epochs 23\n",
            "steps8\n",
            "epochs 24\n",
            "steps8\n",
            "epochs 25\n",
            "steps8\n",
            "epochs 26\n",
            "steps8\n",
            "epochs 27\n",
            "steps8\n",
            "epochs 28\n",
            "steps8\n",
            "epochs 29\n",
            "steps8\n",
            "epochs 30\n",
            "steps8\n",
            "epochs 31\n",
            "steps8\n",
            "epochs 32\n",
            "steps8\n",
            "epochs 33\n",
            "steps8\n",
            "epochs 34\n",
            "steps8\n",
            "epochs 35\n",
            "steps8\n",
            "epochs 36\n",
            "steps8\n",
            "epochs 37\n",
            "steps8\n",
            "epochs 38\n",
            "steps8\n",
            "epochs 39\n",
            "steps8\n",
            "epochs 40\n",
            "steps8\n",
            "epochs 41\n",
            "steps8\n",
            "epochs 42\n",
            "steps8\n",
            "epochs 43\n",
            "steps8\n",
            "epochs 44\n",
            "steps8\n",
            "epochs 45\n",
            "steps8\n",
            "epochs 46\n",
            "steps8\n",
            "epochs 47\n",
            "steps8\n",
            "epochs 48\n",
            "steps8\n",
            "epochs 49\n",
            "steps8\n",
            "epochs 50\n",
            "steps8\n",
            "epochs 51\n",
            "steps8\n",
            "epochs 52\n",
            "steps8\n",
            "epochs 53\n",
            "steps8\n",
            "epochs 54\n",
            "steps8\n",
            "epochs 55\n",
            "steps8\n",
            "epochs 56\n",
            "steps8\n",
            "epochs 57\n",
            "steps8\n",
            "epochs 58\n",
            "steps8\n",
            "epochs 59\n",
            "steps8\n",
            "epochs 60\n",
            "steps8\n",
            "epochs 61\n",
            "steps8\n",
            "epochs 62\n",
            "steps8\n",
            "epochs 63\n",
            "steps8\n",
            "epochs 64\n",
            "steps8\n",
            "epochs 65\n",
            "steps8\n",
            "epochs 66\n",
            "steps8\n",
            "epochs 67\n",
            "steps8\n",
            "epochs 68\n",
            "steps8\n",
            "epochs 69\n",
            "steps8\n",
            "epochs 70\n",
            "steps8\n",
            "epochs 71\n",
            "steps8\n",
            "epochs 72\n",
            "steps8\n",
            "epochs 73\n",
            "steps8\n",
            "epochs 74\n",
            "steps8\n",
            "epochs 75\n",
            "steps8\n",
            "epochs 76\n",
            "steps8\n",
            "epochs 77\n",
            "steps8\n",
            "epochs 78\n",
            "steps8\n",
            "epochs 79\n",
            "steps8\n",
            "epochs 80\n",
            "steps8\n",
            "epochs 81\n",
            "steps8\n",
            "epochs 82\n",
            "steps8\n",
            "epochs 83\n",
            "steps8\n",
            "epochs 84\n",
            "steps8\n",
            "epochs 85\n",
            "steps8\n",
            "epochs 86\n",
            "steps8\n",
            "epochs 87\n",
            "steps8\n",
            "epochs 88\n",
            "steps8\n",
            "epochs 89\n",
            "steps8\n",
            "epochs 90\n",
            "steps8\n",
            "epochs 91\n",
            "steps8\n",
            "epochs 92\n",
            "steps8\n",
            "epochs 93\n",
            "steps8\n",
            "epochs 94\n",
            "steps8\n",
            "epochs 95\n",
            "steps8\n",
            "epochs 96\n",
            "steps8\n",
            "epochs 97\n",
            "steps8\n",
            "epochs 98\n",
            "steps8\n",
            "epochs 99\n",
            "steps8\n",
            "epochs 100\n",
            "steps8\n",
            "Saving checkpoint at epoch100\n",
            "\n",
            "\n",
            "\n",
            "epochs 101\n",
            "steps8\n",
            "epochs 102\n",
            "steps8\n",
            "epochs 103\n",
            "steps8\n",
            "epochs 104\n",
            "steps8\n",
            "epochs 105\n",
            "steps8\n",
            "epochs 106\n",
            "steps8\n",
            "epochs 107\n",
            "steps8\n",
            "epochs 108\n",
            "steps8\n",
            "epochs 109\n",
            "steps8\n",
            "epochs 110\n",
            "steps8\n",
            "epochs 111\n",
            "steps8\n",
            "epochs 112\n",
            "steps8\n",
            "epochs 113\n",
            "steps8\n",
            "epochs 114\n",
            "steps8\n",
            "epochs 115\n",
            "steps8\n",
            "epochs 116\n",
            "steps8\n",
            "epochs 117\n",
            "steps8\n",
            "epochs 118\n",
            "steps8\n",
            "epochs 119\n",
            "steps8\n",
            "epochs 120\n",
            "steps8\n",
            "epochs 121\n",
            "steps8\n",
            "epochs 122\n",
            "steps8\n",
            "epochs 123\n",
            "steps8\n",
            "epochs 124\n",
            "steps8\n",
            "epochs 125\n",
            "steps8\n",
            "epochs 126\n",
            "steps8\n",
            "epochs 127\n",
            "steps8\n",
            "epochs 128\n",
            "steps8\n",
            "epochs 129\n",
            "steps8\n",
            "epochs 130\n",
            "steps8\n",
            "epochs 131\n",
            "steps8\n",
            "epochs 132\n",
            "steps8\n",
            "epochs 133\n",
            "steps8\n",
            "epochs 134\n",
            "steps8\n",
            "epochs 135\n",
            "steps8\n",
            "epochs 136\n",
            "steps8\n",
            "epochs 137\n",
            "steps8\n",
            "epochs 138\n",
            "steps8\n",
            "epochs 139\n",
            "steps8\n",
            "epochs 140\n",
            "steps8\n",
            "epochs 141\n",
            "steps8\n",
            "epochs 142\n",
            "steps8\n",
            "epochs 143\n",
            "steps8\n",
            "epochs 144\n",
            "steps8\n",
            "epochs 145\n",
            "steps8\n",
            "epochs 146\n",
            "steps8\n",
            "epochs 147\n",
            "steps8\n",
            "epochs 148\n",
            "steps8\n",
            "epochs 149\n",
            "steps8\n",
            "epochs 150\n",
            "steps8\n",
            "epochs 151\n",
            "steps8\n",
            "epochs 152\n",
            "steps8\n",
            "epochs 153\n",
            "steps8\n",
            "epochs 154\n",
            "steps8\n",
            "epochs 155\n",
            "steps8\n",
            "epochs 156\n",
            "steps8\n",
            "epochs 157\n",
            "steps8\n",
            "epochs 158\n",
            "steps8\n",
            "epochs 159\n",
            "steps8\n",
            "epochs 160\n",
            "steps8\n",
            "epochs 161\n",
            "steps8\n",
            "epochs 162\n",
            "steps8\n",
            "epochs 163\n",
            "steps8\n",
            "epochs 164\n",
            "steps8\n",
            "epochs 165\n",
            "steps8\n",
            "epochs 166\n",
            "steps8\n",
            "epochs 167\n",
            "steps8\n",
            "epochs 168\n",
            "steps8\n",
            "epochs 169\n",
            "steps8\n",
            "epochs 170\n",
            "steps8\n",
            "epochs 171\n",
            "steps8\n",
            "epochs 172\n",
            "steps8\n",
            "epochs 173\n",
            "steps8\n",
            "epochs 174\n",
            "steps8\n",
            "epochs 175\n",
            "steps8\n",
            "epochs 176\n",
            "steps8\n",
            "epochs 177\n",
            "steps8\n",
            "epochs 178\n",
            "steps8\n",
            "epochs 179\n",
            "steps8\n",
            "epochs 180\n",
            "steps8\n",
            "epochs 181\n",
            "steps8\n",
            "epochs 182\n",
            "steps8\n",
            "epochs 183\n",
            "steps8\n",
            "epochs 184\n",
            "steps8\n",
            "epochs 185\n",
            "steps8\n",
            "epochs 186\n",
            "steps8\n",
            "epochs 187\n",
            "steps8\n",
            "epochs 188\n",
            "steps8\n",
            "epochs 189\n",
            "steps8\n",
            "epochs 190\n",
            "steps8\n",
            "epochs 191\n",
            "steps8\n",
            "epochs 192\n",
            "steps8\n",
            "epochs 193\n",
            "steps8\n",
            "epochs 194\n",
            "steps8\n",
            "epochs 195\n",
            "steps8\n",
            "epochs 196\n",
            "steps8\n",
            "epochs 197\n",
            "steps8\n",
            "epochs 198\n",
            "steps8\n",
            "epochs 199\n",
            "steps8\n",
            "epochs 200\n",
            "steps8\n",
            "Saving checkpoint at epoch200\n",
            "\n",
            "\n",
            "\n",
            "epochs 201\n",
            "steps8\n",
            "epochs 202\n",
            "steps8\n",
            "epochs 203\n",
            "steps8\n",
            "epochs 204\n",
            "steps8\n",
            "epochs 205\n",
            "steps8\n",
            "epochs 206\n",
            "steps8\n",
            "epochs 207\n",
            "steps8\n",
            "epochs 208\n",
            "steps8\n",
            "epochs 209\n",
            "steps8\n",
            "epochs 210\n",
            "steps8\n",
            "epochs 211\n",
            "steps8\n",
            "epochs 212\n",
            "steps8\n",
            "epochs 213\n",
            "steps8\n",
            "epochs 214\n",
            "steps8\n",
            "epochs 215\n",
            "steps8\n",
            "epochs 216\n",
            "steps8\n",
            "epochs 217\n",
            "steps8\n",
            "epochs 218\n",
            "steps8\n",
            "epochs 219\n",
            "steps8\n",
            "epochs 220\n",
            "steps8\n",
            "epochs 221\n",
            "steps8\n",
            "epochs 222\n",
            "steps8\n",
            "epochs 223\n",
            "steps8\n",
            "epochs 224\n",
            "steps8\n",
            "epochs 225\n",
            "steps8\n",
            "epochs 226\n",
            "steps8\n",
            "epochs 227\n",
            "steps8\n",
            "epochs 228\n",
            "steps8\n",
            "epochs 229\n",
            "steps8\n",
            "epochs 230\n",
            "steps8\n",
            "epochs 231\n",
            "steps8\n",
            "epochs 232\n",
            "steps8\n",
            "epochs 233\n",
            "steps8\n",
            "epochs 234\n",
            "steps8\n",
            "epochs 235\n",
            "steps8\n",
            "epochs 236\n",
            "steps8\n",
            "epochs 237\n",
            "steps8\n",
            "epochs 238\n",
            "steps8\n",
            "epochs 239\n",
            "steps8\n",
            "epochs 240\n",
            "steps8\n",
            "epochs 241\n",
            "steps8\n",
            "epochs 242\n",
            "steps8\n",
            "epochs 243\n",
            "steps8\n",
            "epochs 244\n",
            "steps8\n",
            "epochs 245\n",
            "steps8\n",
            "epochs 246\n",
            "steps8\n",
            "epochs 247\n",
            "steps8\n",
            "epochs 248\n",
            "steps8\n",
            "epochs 249\n",
            "steps8\n",
            "epochs 250\n",
            "steps8\n",
            "epochs 251\n",
            "steps8\n",
            "epochs 252\n",
            "steps8\n",
            "epochs 253\n",
            "steps8\n",
            "epochs 254\n",
            "steps8\n",
            "epochs 255\n",
            "steps8\n",
            "epochs 256\n",
            "steps8\n",
            "epochs 257\n",
            "steps8\n",
            "epochs 258\n",
            "steps8\n",
            "epochs 259\n",
            "steps8\n",
            "epochs 260\n",
            "steps8\n",
            "epochs 261\n",
            "steps8\n",
            "epochs 262\n",
            "steps8\n",
            "epochs 263\n",
            "steps8\n",
            "epochs 264\n",
            "steps8\n",
            "epochs 265\n",
            "steps8\n",
            "epochs 266\n",
            "steps8\n",
            "epochs 267\n",
            "steps8\n",
            "epochs 268\n",
            "steps8\n",
            "epochs 269\n",
            "steps8\n",
            "epochs 270\n",
            "steps8\n",
            "epochs 271\n",
            "steps8\n",
            "epochs 272\n",
            "steps8\n",
            "epochs 273\n",
            "steps8\n",
            "epochs 274\n",
            "steps8\n",
            "epochs 275\n",
            "steps8\n",
            "epochs 276\n",
            "steps8\n",
            "epochs 277\n",
            "steps1"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}